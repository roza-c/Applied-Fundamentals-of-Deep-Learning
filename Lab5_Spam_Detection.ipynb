{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bphECiUa9zw"
      },
      "source": [
        "# Lab 5: Spam Detection\n",
        "\n",
        "In this assignment, we will build a recurrent neural network to classify a SMS text message\n",
        "as \"spam\" or \"not spam\". In the process, you will\n",
        "    \n",
        "1. Clean and process text data for machine learning.\n",
        "2. Understand and implement a character-level recurrent neural network.\n",
        "3. Use torchtext to build recurrent neural network models.\n",
        "4. Understand batching for a recurrent neural network, and use torchtext to implement RNN batching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL9LMfPU6Clp"
      },
      "source": [
        "Colab Link:https://colab.research.google.com/drive/1M0TAcacWcxSti2co7sAfEg16wzAHFnlI?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "fGWwUl4I-Jla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIjxZ1l06Cls"
      },
      "source": [
        "If you are interested to use the most recent version if torchtext, you can look at the following document to see how to convert the legacy version to the new version:\n",
        "https://colab.research.google.com/github/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgfNOUaPa9z8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6 torch==1.11"
      ],
      "metadata": {
        "id": "cDHxBnTc6UUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jLI9LBa90C"
      },
      "source": [
        "## Part 1. Data Cleaning [15 pt]\n",
        "\n",
        "We will be using the \"SMS Spam Collection Data Set\" available at http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
        "\n",
        "There is a link to download the \"Data Folder\" at the very top of the webpage. Download the zip file, unzip it, and upload the file `SMSSpamCollection` to Colab.    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SMSSpamCollection = '/content/gdrive/MyDrive/Colab Notebooks/SMS_Spam_Collection/SMSSpamCollection'"
      ],
      "metadata": {
        "id": "yBr3uCXz-8Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSuF7C_Ga90E"
      },
      "source": [
        "### Part (a) [2 pt]\n",
        "\n",
        "Open up the file in Python, and print out one example of a spam SMS, and one example of a non-spam SMS.\n",
        "\n",
        "What is the label value for a spam message, and what is the label value for a non-spam message?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_IfXHeTa90F",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef47fcd-17f5-4be7-96e2-3160cc84fcea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam message example: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "\n",
            "Spam label: spam\n",
            "\n",
            "Non-spam message example: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "\n",
            "Non-spam label: ham\n"
          ]
        }
      ],
      "source": [
        "for line in open(SMSSpamCollection):\n",
        "    if line.split()[0] == 'spam':\n",
        "        print('Spam message example:', line[5:])\n",
        "        print('Spam label:', line.split()[0])\n",
        "        print('')\n",
        "        break\n",
        "\n",
        "for line in open(SMSSpamCollection):\n",
        "    if line.split()[0] == 'ham':\n",
        "        print('Non-spam message example:', line[4:])\n",
        "        print('Non-spam label:', line.split()[0])\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AukA6vMVa90d"
      },
      "source": [
        "### Part (b) [1 pt]\n",
        "\n",
        "How many spam messages are there in the data set?\n",
        "How many non-spam messages are there in the data set?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgsqyemVa90e",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e6852d8-9a25-40ae-f689-a039774c6181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 747 spam messages\n",
            "There are 4827 non-spam messages\n"
          ]
        }
      ],
      "source": [
        "spam_count = 0\n",
        "ham_count = 0\n",
        "for msg in open(SMSSpamCollection):\n",
        "  if msg.split()[0] == 'spam':\n",
        "        spam_count += 1\n",
        "  elif msg.split()[0] == 'ham':\n",
        "      ham_count += 1\n",
        "print('There are', spam_count, 'spam messages')\n",
        "print('There are', ham_count, 'non-spam messages')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1WXxVt6a90h"
      },
      "source": [
        "### Part (c) [4 pt]\n",
        "\n",
        "We will be using the package `torchtext` to load, process, and batch the data.\n",
        "A tutorial to torchtext is available below. This tutorial uses the same\n",
        "Sentiment140 data set that we explored during lecture.\n",
        "\n",
        "https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8\n",
        "\n",
        "Unlike what we did during lecture, we will be building a **character level RNN**.\n",
        "That is, we will treat each **character** as a token in our sequence,\n",
        "rather than each **word**.\n",
        "\n",
        "Identify two advantage and two disadvantage of modelling SMS text\n",
        "messages as a sequence of characters rather than a sequence of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Mhnz8Nk-a90i",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Advantages\n",
        "\n",
        "  # 1) treating each character as a token requires less memory since there are\n",
        "  # much less characters than there are words\n",
        "  # 2) modellings SMS texts as a sequence of characters instead of words means\n",
        "  # that the potential of misspelling words is reduced\n",
        "  # the model is better at identifying patterns within words\n",
        "\n",
        "# Disadvantages\n",
        "\n",
        "  # 1) higher computational cost since modelling SMS texts as a sequence of\n",
        "  # characters would require more hidden layers\n",
        "  # 2) lower accuracy compared to word level RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie_D0bv9a90k"
      },
      "source": [
        "### Part (d) [1 pt]\n",
        "\n",
        "We will be loading our data set using `torchtext.data.TabularDataset`. The\n",
        "constructor will read directly from the `SMSSpamCollection` file.\n",
        "\n",
        "For the data file to be read successfuly, we\n",
        "need to specify the **fields** (columns) in the file.\n",
        "In our case, the dataset has two fields:\n",
        "\n",
        "- a text field containing the sms messages,\n",
        "- a label field which will be converted into a binary label.\n",
        "\n",
        "Split the dataset into `train`, `valid`, and `test`. Use a 60-20-20 split.\n",
        "You may find this torchtext API page helpful:\n",
        "https://torchtext.readthedocs.io/en/latest/data.html#dataset\n",
        "\n",
        "Hint: There is a `Dataset` method that can perform the random split for you."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "\n",
        "text_field = torchtext.data.Field(sequential=True,\n",
        "                       tokenize=lambda x: x,\n",
        "                       include_lengths=True,\n",
        "                       batch_first = True,\n",
        "                       use_vocab=True)\n",
        "\n",
        "label_field = torchtext.data.Field(sequential=False,\n",
        "                                   use_vocab=False,\n",
        "                                   is_target=True,\n",
        "                                   batch_first=True,\n",
        "                                   preprocessing=lambda x: int(x == 'spam'))\n",
        "\n",
        "fields = [('label', label_field), ('sms', text_field)]\n",
        "\n",
        "dataset = torchtext.data.TabularDataset(SMSSpamCollection,\n",
        "                                        \"tsv\",\n",
        "                                        fields)\n",
        "\n",
        "training_data, validation_data, testing_data = dataset.split([0.60, 0.20, 0.20], True)"
      ],
      "metadata": {
        "id": "4svfYBcirbtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6nP0Ks_a90o"
      },
      "source": [
        "### Part (e) [2 pt]\n",
        "\n",
        "You saw in part (b) that there are many more non-spam messages than spam messages.\n",
        "This **imbalance** in our training data will be problematic for training.\n",
        "We can fix this disparity by duplicating spam messages in the training set,\n",
        "so that the training set is roughly **balanced**.\n",
        "\n",
        "Explain why having a balanced training set is helpful for training our neural network.\n",
        "\n",
        "Note: if you are not sure, try removing the below code and train your mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FWvx9_rka90p",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# save the original training examples\n",
        "old_train_examples = training_data.examples\n",
        "# get all the spam messages in `train`\n",
        "train_spam = []\n",
        "for item in training_data.examples:\n",
        "    if item.label == 1:\n",
        "        train_spam.append(item)\n",
        "# duplicate each spam message 6 more times\n",
        "training_data.examples = old_train_examples + train_spam * 6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# having a balanced training set is helpful for training our neural network\n",
        "# because we don't want the model to be biased towards the class which has\n",
        "# more training data. if the dataset is imbalanced, the model will learn more\n",
        "# features of one class and less of the others, result in a model which might\n",
        "# not generalize well, thus overfitting and also low accuracy"
      ],
      "metadata": {
        "id": "_i6OQBzGsJkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7eUmBEva90r"
      },
      "source": [
        "### Part (f) [1 pt]\n",
        "\n",
        "We need to build the vocabulary on the training data by running the below code.\n",
        "This finds all the possible character tokens in the training set.\n",
        "\n",
        "Explain what the variables `text_field.vocab.stoi` and `text_field.vocab.itos` represent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CQM8flKa90s",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e14d85-66c5-4234-beea-a05a349e7332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7db044bb70d0>>, {'<unk>': 0, '<pad>': 1, ' ': 2, 'e': 3, 'o': 4, 't': 5, 'a': 6, 'n': 7, 'r': 8, 'i': 9, 's': 10, 'l': 11, 'u': 12, 'h': 13, '0': 14, 'd': 15, 'c': 16, '.': 17, 'm': 18, 'y': 19, 'w': 20, 'p': 21, 'g': 22, '1': 23, 'f': 24, 'b': 25, '2': 26, 'T': 27, '8': 28, 'k': 29, 'E': 30, 'v': 31, '5': 32, 'S': 33, 'C': 34, 'O': 35, 'I': 36, '4': 37, 'N': 38, '7': 39, 'A': 40, 'x': 41, '3': 42, '6': 43, 'R': 44, '!': 45, '9': 46, ',': 47, 'P': 48, 'M': 49, 'W': 50, 'L': 51, 'U': 52, 'H': 53, 'D': 54, 'B': 55, 'F': 56, 'G': 57, 'Y': 58, \"'\": 59, '/': 60, '?': 61, '£': 62, '&': 63, '-': 64, 'X': 65, ':': 66, 'z': 67, 'V': 68, 'j': 69, 'K': 70, '*': 71, 'J': 72, ')': 73, '+': 74, ';': 75, '(': 76, '\"': 77, 'q': 78, 'Q': 79, '>': 80, '#': 81, '@': 82, '=': 83, 'Z': 84, 'ü': 85, 'Ü': 86, '$': 87, '‘': 88, '\\x92': 89, '[': 90, ']': 91, '<': 92, '%': 93, '_': 94, '|': 95, '¡': 96, '’': 97, '…': 98, '\\x93': 99, '–': 100, 'é': 101, '\\t': 102, '\\n': 103, '\\\\': 104, '\\x91': 105, '\\x94': 106, '\\x96': 107, '^': 108, '~': 109, '»': 110, 'É': 111, 'ì': 112, '┾': 113, '〨': 114, '鈥': 115})\n",
            "['<unk>', '<pad>', ' ', 'e', 'o', 't', 'a', 'n', 'r', 'i', 's', 'l', 'u', 'h', '0', 'd', 'c', '.', 'm', 'y', 'w', 'p', 'g', '1', 'f', 'b', '2', 'T', '8', 'k', 'E', 'v', '5', 'S', 'C', 'O', 'I', '4', 'N', '7', 'A', 'x', '3', '6', 'R', '!', '9', ',', 'P', 'M', 'W', 'L', 'U', 'H', 'D', 'B', 'F', 'G', 'Y', \"'\", '/', '?', '£', '&', '-', 'X', ':', 'z', 'V', 'j', 'K', '*', 'J', ')', '+', ';', '(', '\"', 'q', 'Q', '>', '#', '@', '=', 'Z', 'ü', 'Ü', '$', '‘', '\\x92', '[', ']', '<', '%', '_', '|', '¡', '’', '…', '\\x93', '–', 'é', '\\t', '\\n', '\\\\', '\\x91', '\\x94', '\\x96', '^', '~', '»', 'É', 'ì', '┾', '〨', '鈥']\n"
          ]
        }
      ],
      "source": [
        "text_field.build_vocab(training_data)\n",
        "print(text_field.vocab.stoi)\n",
        "print(text_field.vocab.itos)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text_field.vocab.stoi represents a dictionary mappign of characters which\n",
        "# correspond to their respective numerical identifies\n",
        "# stoi means string to index, maps string token to integer index\n",
        "\n",
        "#text_field.vocab.itos is a list of character tokens which have been\n",
        "# indexed by their corresponding numerical identifiers\n",
        "# itos stands for index to string, maps index to string token"
      ],
      "metadata": {
        "id": "1cIN8NgdtEBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC8WVE8Ua90u"
      },
      "source": [
        "### Part (g) [2 pt]\n",
        "\n",
        "The tokens `<unk>` and `<pad>` were not in our SMS text messages.\n",
        "What do these two values represent?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_4Er7KUa90v",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# <unk> are unknown text tokens in the SMS text message\n",
        "# unknown vocabulary\n",
        "\n",
        "# <pad> are padding token are used to increase the length of the character\n",
        "# sequence to ensure all sequences have the same length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff5CNk7Qa90y"
      },
      "source": [
        "### Part (h) [2 pt]\n",
        "\n",
        "Since text sequences are of variable length, `torchtext` provides a `BucketIterator` data loader,\n",
        "which batches similar length sequences together. The iterator also provides functionalities to\n",
        "pad sequences automatically.\n",
        "\n",
        "Take a look at 10 batches in `train_iter`. What is the maximum length of the\n",
        "input sequence in each batch? How many `<pad>` tokens are used in each of the 10\n",
        "batches?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V8N8qLWOa90y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "train_iter = torchtext.data.BucketIterator(training_data,\n",
        "                                           batch_size=32,\n",
        "                                           sort_key=lambda x: len(x.sms), # to minimize padding\n",
        "                                           sort_within_batch=True,        # sort within each batch\n",
        "                                           repeat=False)                  # repeat the iterator for many epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwz-rOaha902",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "batch_num = 1\n",
        "for batch in train_iter:\n",
        "  if batch_num <= 10:\n",
        "    print('batch number', batch_num, 'has max length:', int(batch.sms[1][0]))\n",
        "    pad = 0\n",
        "    for msg in range(0, len(batch.sms[1])):\n",
        "      pad = pad + (batch.sms[1][0] - batch.sms[1][msg])\n",
        "    print('batch number', batch_num, 'has', int(pad), '<pad> tokens \\n')\n",
        "    batch_num += 1\n",
        "  else:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7HnqP6_a904"
      },
      "source": [
        "## Part 2. Model Building [8 pt]\n",
        "\n",
        "Build a recurrent neural network model, using an architecture of your choosing.\n",
        "Use the one-hot embedding of each character as input to your recurrent network.\n",
        "Use one or more fully-connected layers to make the prediction based on your\n",
        "recurrent network output.\n",
        "\n",
        "Instead of using the RNN output value for the final token, another often used\n",
        "strategy is to max-pool over the entire output array. That is, instead of calling\n",
        "something like:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "self.fc(out[:, -1, :])\n",
        "```\n",
        "\n",
        "where `self.rnn` is an `nn.RNN`, `nn.GRU`, or `nn.LSTM` module, and `self.fc` is a\n",
        "fully-connected\n",
        "layer, we use:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "self.fc(torch.max(out, dim=1)[0])\n",
        "```\n",
        "\n",
        "This works reasonably in practice. An even better alternative is to concatenate the\n",
        "max-pooling and average-pooling of the RNN outputs:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "out = torch.cat([torch.max(out, dim=1)[0],\n",
        "                 torch.mean(out, dim=1)], dim=1)\n",
        "self.fc(out)\n",
        "```\n",
        "\n",
        "We encourage you to try out all these options. The way you pool the RNN outputs\n",
        "is one of the \"hyperparameters\" that you can choose to tune later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHl1p_Wwa905",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec17a990-e980-4158-e914-49f2be0f23dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ],
      "source": [
        "# You might find this code helpful for obtaining\n",
        "# PyTorch one-hot vectors.\n",
        "\n",
        "ident = torch.eye(10)\n",
        "print(ident[0]) # one-hot vector\n",
        "print(ident[1]) # one-hot vector\n",
        "x = torch.tensor([[1, 2], [3, 4]])\n",
        "print(ident[x]) # one-hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_size = len(text_field.vocab.itos)"
      ],
      "metadata": {
        "id": "piXHCTbTw9lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4LTQ7zFka909",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, onehot_size, hidden_size, num_classes):\n",
        "        self.name = \"Spam_Detection_RNN\"\n",
        "        super(RNN, self).__init__()\n",
        "        self.emb = torch.eye(onehot_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(onehot_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb[x]\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(torch.max(out, dim=1)[0])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKIYPl_Ba90_"
      },
      "source": [
        "## Part 3. Training [16 pt]\n",
        "\n",
        "### Part (a) [4 pt]\n",
        "\n",
        "Complete the `get_accuracy` function, which will compute the\n",
        "accuracy (rate) of your model across a dataset (e.g. validation set).\n",
        "You may modify `torchtext.data.BucketIterator` to make your computation\n",
        "faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pvNfhGD6a91A",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, data):\n",
        "    \"\"\" Compute the accuracy of the `model` across a dataset `data`\n",
        "\n",
        "    Example usage:\n",
        "\n",
        "    >>> model = MyRNN() # to be defined\n",
        "    >>> get_accuracy(model, valid) # the variable `valid` is from above\n",
        "    \"\"\"\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    data_loader = torchtext.data.BucketIterator(data, batch_size=64,\n",
        "                                               sort_key=lambda x: len(x.sms),\n",
        "                                               sort_within_batch=True,\n",
        "                                               repeat=False)\n",
        "\n",
        "    for message, label in data_loader:\n",
        "        output = model(message[0])\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "        total += message[0].shape[0]\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlxlcAC1a91C"
      },
      "source": [
        "### Part (b) [4 pt]\n",
        "\n",
        "Train your model. Plot the training curve of your final model.\n",
        "Your training curve should have the training/validation loss and\n",
        "accuracy plotted periodically.\n",
        "\n",
        "Note: Not all of your batches will have the same batch size.\n",
        "In particular, if your training set does not divide evenly by\n",
        "your batch size, there will be a batch that is smaller than\n",
        "the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CVtf7CJCa91D",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def train_RNN(model, training_data, validation_data, batch_size, num_epochs=10, learning_rate=0.0005):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  training_losses, validation_losses, training_accuracy, validation_accuracy = [], [], [], []\n",
        "  epochs = []\n",
        "  train_loader = torchtext.data.BucketIterator(training_data, batch_size=batch_size,\n",
        "                                               sort_key=lambda x: len(x.sms),\n",
        "                                               sort_within_batch=True,\n",
        "                                               repeat=False)\n",
        "  validation_loader = torchtext.data.BucketIterator(validation_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               sort_key=lambda x: len(x.sms),\n",
        "                                               sort_within_batch=True,\n",
        "                                               repeat=False)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for msg, labels in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      prediction = model(msg[0])\n",
        "      loss = criterion(prediction, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    training_losses.append(float(loss))\n",
        "\n",
        "    for msg, labels in validation_loader:\n",
        "      prediction = model(msg[0])\n",
        "      loss = criterion(prediction, labels)\n",
        "    validation_losses.append(float(loss))\n",
        "\n",
        "    epochs.append(epoch)\n",
        "    training_accuracy.append(get_accuracy(model, training_data))\n",
        "    validation_accuracy.append(get_accuracy(model, validation_data))\n",
        "    print(\"Epoch %d; Loss %f; Training Accuracy %f; Validation Accuracy %f\" % (\n",
        "          epoch+1, loss, training_accuracy[-1], validation_accuracy[-1]))\n",
        "    model_path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name, batch_size, learning_rate, epoch)\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "  # plotting\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(training_losses, label=\"Train\")\n",
        "  plt.plot(validation_losses, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Training Curve\")\n",
        "  plt.plot(epochs, training_accuracy, label=\"Train\")\n",
        "  plt.plot(epochs, validation_accuracy, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Highest Training Accuracy: {}\".format(max(training_accuracy)))\n",
        "  print(\"Highest Training Accuracy: {}\".format(max(validation_accuracy)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = RNN(onehot_size, 100, 2)\n",
        "train_RNN(model_1, training_data, validation_data, batch_size=64, num_epochs=25, learning_rate=0.0005)"
      ],
      "metadata": {
        "id": "nvFR19ZOxvWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE3eRkDAa91F"
      },
      "source": [
        "### Part (c) [4 pt]\n",
        "\n",
        "Choose at least 4 hyperparameters to tune. Explain how you tuned the hyperparameters.\n",
        "You don't need to include your training curve for every model you trained.\n",
        "Instead, explain what hyperparemters you tuned, what the best validation accuracy was,\n",
        "and the reasoning behind the hyperparameter decisions you made.\n",
        "\n",
        "For this assignment, you should tune more than just your learning rate and epoch.\n",
        "Choose at least 2 hyperparameters that are unrelated to the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2GEWfDca91G",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# I will first change the number of hidden layers between the first fully connected\n",
        "# layer and the second fully connected layer. Since the model classifies whether\n",
        "# the message is spam or not spam, changing the number of hidden layers helps to\n",
        "# tune the classifiers performance. I doubled it from 100->200\n",
        "\n",
        "model_2 = RNN(onehot_size, 200, 2)\n",
        "train_RNN(model_2, training_data, validation_data, batch_size=64, num_epochs=25, learning_rate=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I will now change the batch size to try to reduce noise\n",
        "# Also reduce the number of epochs since the model seems to be overfitting a bit\n",
        "# increased batch size 64->128, decreased epochs 25->20\n",
        "\n",
        "model_3 = RNN(onehot_size, 200, 2)\n",
        "train_RNN(model_3, training_data, validation_data, batch_size=128, num_epochs=20, learning_rate=0.0005)"
      ],
      "metadata": {
        "id": "UcRHFUBnVMMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Too much overfitting, try decreasing batch size to 32\n",
        "\n",
        "model_4 = RNN(onehot_size, 200, 2)\n",
        "train_RNN(model_4, training_data, validation_data, batch_size=32, num_epochs=20, learning_rate=0.0005)"
      ],
      "metadata": {
        "id": "vsADiNiBdIHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I will keep batch size at 64, previous is overfitting too.\n",
        "# Now I will decrease learning rate to try to improve model performance. Decreased from 0.0005->0.0001\n",
        "\n",
        "model_5 = RNN(onehot_size, 200, 2)\n",
        "train_RNN(model_5, training_data, validation_data, batch_size=64, num_epochs=25, learning_rate=0.0001)"
      ],
      "metadata": {
        "id": "AflSMV2jfM93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now I will try increasing learning rate to try to improve model performance.\n",
        "# Decreased from 0.0001->0.001\n",
        "\n",
        "model_6 = RNN(onehot_size, 200, 2)\n",
        "train_RNN(model_6, training_data, validation_data, batch_size=64, num_epochs=20, learning_rate=0.001)"
      ],
      "metadata": {
        "id": "n0QsfRGyjO71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I will keep learning rate at 0.0005.\n",
        "# Lastly I will use a new RNN model with output = self.fc(out[:, -1, :])"
      ],
      "metadata": {
        "id": "CpJzLH6yk05w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, onehot_size, hidden_size, num_classes):\n",
        "        self.name = \"Spam_Detection_RNN\"\n",
        "        super(RNN, self).__init__()\n",
        "        self.emb = torch.eye(onehot_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(onehot_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb[x]\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        output, _ = self.rnn(x, h0)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output"
      ],
      "metadata": {
        "id": "EoHGQmQkljH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_7 = RNN(onehot_size, 200, 2)\n",
        "train_RNN(model_7, training_data, validation_data, batch_size=64, num_epochs=20, learning_rate=0.0005)"
      ],
      "metadata": {
        "id": "vMNApPvrlsYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I will retrain model_2 with 20 epochs instead and that will be the best model\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, onehot_size, hidden_size, num_classes):\n",
        "        self.name = \"Spam_Detection_RNN\"\n",
        "        super(RNN, self).__init__()\n",
        "        self.emb = torch.eye(onehot_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(onehot_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb[x]\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        output, _ = self.rnn(x, h0)\n",
        "        output = self.fc(torch.max(output, dim=1)[0])\n",
        "        return output"
      ],
      "metadata": {
        "id": "gio-JjX_roz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_8 = RNN(onehot_size, 200, 2)\n",
        "train_RNN(model_8, training_data, validation_data, batch_size=64, num_epochs=20, learning_rate=0.0005)"
      ],
      "metadata": {
        "id": "JeNyrHa2r633"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7DY56rKa91I"
      },
      "source": [
        "### Part (d) [2 pt]\n",
        "\n",
        "Before we deploy a machine learning model, we usually want to have a better understanding\n",
        "of how our model performs beyond its validation accuracy. An important metric to track is\n",
        "*how well our model performs in certain subsets of the data*.\n",
        "\n",
        "In particular, what is the model's error rate amongst data with negative labels?\n",
        "This is called the **false positive rate**.\n",
        "\n",
        "What about the model's error rate amongst data with positive labels?\n",
        "This is called the **false negative rate**.\n",
        "\n",
        "Report your final model's false positive and false negative rate across the\n",
        "validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ggbQSdba91J",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4d953e-30c9-4d7b-a81c-30497c6d156b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The false positive rate is: 0.7253886010362698 %\n",
            "The false negative rate is: 7.9999999999999964 %\n"
          ]
        }
      ],
      "source": [
        "# Create a Dataset of only spam validation examples\n",
        "valid_spam = torchtext.data.Dataset(\n",
        "    [e for e in validation_data.examples if e.label == 1],\n",
        "    validation_data.fields)\n",
        "\n",
        "# Create a Dataset of only non-spam validation examples\n",
        "valid_nospam = torchtext.data.Dataset(\n",
        "    [e for e in validation_data.examples if e.label == 0],\n",
        "    validation_data.fields)\n",
        "\n",
        "\n",
        "valid_spam_accuracy = get_accuracy(model_8, valid_spam)\n",
        "valid_nospam_accuracy = get_accuracy(model_8, valid_nospam)\n",
        "\n",
        "false_positive_rate = (1 - valid_nospam_accuracy)*100\n",
        "false_negative_rate = (1 - valid_spam_accuracy)*100\n",
        "\n",
        "print(\"The false positive rate is:\", false_positive_rate, \"%\")\n",
        "print(\"The false negative rate is:\", false_negative_rate, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1iRteb3a91O"
      },
      "source": [
        "### Part (e) [2 pt]\n",
        "\n",
        "The impact of a false positive vs a false negative can be drastically different.\n",
        "If our spam detection algorithm was deployed on your phone, what is the impact\n",
        "of a false positive on the phone's user? What is the impact of a false negative?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hFLUOJTGa91Q",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# A false positive means that a non-spam message was classified as a spam message.\n",
        "# If our spam detection algorithm was deployed on a phone, it could mean that\n",
        "# the user might miss important messages since they were classified and sorted\n",
        "# into the spam folder instead of into their messages.\n",
        "\n",
        "# A false negative means that a spam message was classified as a non-spam message.\n",
        "# For the phone's user, this could mean that the user is receiving a lot of spam\n",
        "# messages or even malicious/phishing emails which could harm the user or their\n",
        "# phone. This can be annoying to the user since they are constantly getting\n",
        "# messages which they have to spend time deciding if they're valid or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gznefulsa91V"
      },
      "source": [
        "## Part 4. Evaluation [11 pt]\n",
        "\n",
        "### Part (a) [1 pt]\n",
        "\n",
        "Report the final test accuracy of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5L5D-A1a91W",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a398803f-eb40-4566-cb66-eb2c4461e179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best performing model's test accuracy is:  0.9757630161579892\n"
          ]
        }
      ],
      "source": [
        "testing_accuracy = get_accuracy(model_8, testing_data)\n",
        "print(\"The best performing model's test accuracy is: \", testing_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hjmd8rca91Y"
      },
      "source": [
        "### Part (b) [3 pt]\n",
        "\n",
        "Report the false positive rate and false negative rate of your model across the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_spam = torchtext.data.Dataset(\n",
        "    [e for e in testing_data.examples if e.label == 1],\n",
        "    testing_data.fields)\n",
        "\n",
        "test_nospam = torchtext.data.Dataset(\n",
        "    [e for e in testing_data.examples if e.label == 0],\n",
        "    testing_data.fields)\n",
        "\n",
        "test_spam_accuracy = get_accuracy(model_8, testing_spam)\n",
        "test_nospam_accuracy = get_accuracy(model_8, test_nospam)\n",
        "\n",
        "false_positive_rate = (1 - test_nospam_accuracy)*100\n",
        "false_negative_rate = (1 - test_spam_accuracy)*100\n",
        "\n",
        "print(\"The false positive rate is:\", false_positive_rate, \"%\")\n",
        "print(\"The false negative rate is:\", false_negative_rate, \"%\")"
      ],
      "metadata": {
        "id": "yILJNNEv3vOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e332ff4-8922-46fd-b1d5-f2b3b125b89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The false positive rate is: 1.2435233160621784 %\n",
            "The false negative rate is: 10.738255033557042 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jGHtQFpa91b"
      },
      "source": [
        "### Part (c) [3 pt]\n",
        "\n",
        "What is your model's prediction of the **probability** that\n",
        "the SMS message \"machine learning is sooo cool!\" is spam?\n",
        "\n",
        "Hint: To begin, use `text_field.vocab.stoi` to look up the index\n",
        "of each character in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_2nSJq8a91b",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4274e36a-5b6e-48c9-9289-839be0c03af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability the message is spam is 2.230325222015381 %\n"
          ]
        }
      ],
      "source": [
        "msg = \"machine learning is sooo cool!\"\n",
        "\n",
        "msg_index = []\n",
        "for char in msg:\n",
        "  msg_index.append(text_field.vocab.stoi[char])\n",
        "\n",
        "test_msg = torch.LongTensor([msg_index])\n",
        "prediction = model_8(test_msg)\n",
        "probability = F.softmax(prediction, dim=1)\n",
        "print(\"The probability the message is spam is\", float(probability[0][1]*100), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD1zgYJpa91f"
      },
      "source": [
        "### Part (d) [4 pt]\n",
        "\n",
        "Do you think detecting spam is an easy or difficult task?\n",
        "\n",
        "Since machine learning models are expensive to train and deploy, it is very\n",
        "important to compare our models against baseline models: a simple\n",
        "model that is easy to build and inexpensive to run that we can compare our\n",
        "recurrent neural network model against.\n",
        "\n",
        "Explain how you might build a simple baseline model. This baseline model\n",
        "can be a simple neural network (with very few weights), a hand-written algorithm,\n",
        "or any other strategy that is easy to build and test.\n",
        "\n",
        "**Do not actually build a baseline model. Instead, provide instructions on\n",
        "how to build it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LTndp-IOa91g",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Compared to computer vision taks, one might think detecting spam is a relatively\n",
        "# easy task, however detecting spam is actually a very diffcult task since spam\n",
        "# messages use manipulative language and try to target vulnerable users'\n",
        "# emotions to try to make it seem like a human sent the message.\n",
        "# As spam messages continue to get more convincing as more language is\n",
        "# introduced and understood online, it gets harder to detect spam.\n",
        "\n",
        "# This is how I would build a simple baseline model.\n",
        "# I would build it using a lot of words/symbols/emoticons that commonly\n",
        "# occur in spam message to use as an indicator for whether the message might\n",
        "# be spam or not. If the content of the message passes a certain threshold of\n",
        "# potential spam phrases, it has a higher probability of being spam. We can\n",
        "# calculate the spam probability for each message ans use that probability for\n",
        "# the baseline model. While this would not be as accurate as an RNN, it\n",
        "# would be a lot simpler to build and test, hence could be used as a good\n",
        "# baseline to ensure the RNN's performance exceeds the baseline model's accuracy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}