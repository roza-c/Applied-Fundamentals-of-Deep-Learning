{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6WDvajSqIDs"
      },
      "source": [
        "# Lab 2: Cats vs Dogs\n",
        "\n",
        "In this lab, you will train a convolutional neural network to classify an image\n",
        "into one of two classes: \"cat\" or \"dog\". The code for the neural networks\n",
        "you train will be written for you, and you are not (yet!) expected\n",
        "to understand all provided code. However, by the end of the lab,\n",
        "you should be able to:\n",
        "\n",
        "1. Understand at a high level the training loop for a machine learning model.\n",
        "2. Understand the distinction between training, validation, and test data.\n",
        "3. The concepts of overfitting and underfitting.\n",
        "4. Investigate how different hyperparameters, such as learning rate and batch size, affect the success of training.\n",
        "5. Compare an ANN (aka Multi-Layer Perceptron) with a CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SwyDuiuUqIDv",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvTXpH_kqIDy"
      },
      "source": [
        "## Part 0. Helper Functions\n",
        "\n",
        "We will be making use of the following helper functions. You will be asked to look\n",
        "at and possibly modify some of these, but you are not expected to understand all of them.\n",
        "\n",
        "You should look at the function names and read the docstrings. If you are curious, come back and explore the code *after* making some progress on the lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wEjwWEkoqIDz",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# Data Loading\n",
        "\n",
        "def get_relevant_indices(dataset, classes, target_classes):\n",
        "    \"\"\" Return the indices for datapoints in the dataset that belongs to the\n",
        "    desired target classes, a subset of all possible classes.\n",
        "\n",
        "    Args:\n",
        "        dataset: Dataset object\n",
        "        classes: A list of strings denoting the name of each class\n",
        "        target_classes: A list of strings denoting the name of desired classes\n",
        "                        Should be a subset of the 'classes'\n",
        "    Returns:\n",
        "        indices: list of indices that have labels corresponding to one of the\n",
        "                 target classes\n",
        "    \"\"\"\n",
        "    indices = []\n",
        "    for i in range(len(dataset)):\n",
        "        # Check if the label is in the target classes\n",
        "        label_index = dataset[i][1] # ex: 3\n",
        "        label_class = classes[label_index] # ex: 'cat'\n",
        "        if label_class in target_classes:\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "def get_data_loader(target_classes, batch_size):\n",
        "    \"\"\" Loads images of cats and dogs, splits the data into training, validation\n",
        "    and testing datasets. Returns data loaders for the three preprocessed datasets.\n",
        "\n",
        "    Args:\n",
        "        target_classes: A list of strings denoting the name of the desired\n",
        "                        classes. Should be a subset of the argument 'classes'\n",
        "        batch_size: A int representing the number of samples per batch\n",
        "\n",
        "    Returns:\n",
        "        train_loader: iterable training dataset organized according to batch size\n",
        "        val_loader: iterable validation dataset organized according to batch size\n",
        "        test_loader: iterable testing dataset organized according to batch size\n",
        "        classes: A list of strings denoting the name of each class\n",
        "    \"\"\"\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    ########################################################################\n",
        "    # The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "    # We transform them to Tensors of normalized range [-1, 1].\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    # Load CIFAR10 training data\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "    # Get the list of indices to sample from\n",
        "    relevant_indices = get_relevant_indices(trainset, classes, target_classes)\n",
        "\n",
        "    # Split into train and validation\n",
        "    np.random.seed(1000) # Fixed numpy random seed for reproducible shuffling\n",
        "    np.random.shuffle(relevant_indices)\n",
        "    split = int(len(relevant_indices) * 0.8) #split at 80%\n",
        "\n",
        "    # split into training and validation indices\n",
        "    relevant_train_indices, relevant_val_indices = relevant_indices[:split], relevant_indices[split:]\n",
        "    train_sampler = SubsetRandomSampler(relevant_train_indices)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                               num_workers=1, sampler=train_sampler)\n",
        "    val_sampler = SubsetRandomSampler(relevant_val_indices)\n",
        "    val_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                              num_workers=1, sampler=val_sampler)\n",
        "    # Load CIFAR10 testing data\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True, transform=transform)\n",
        "    # Get the list of indices to sample from\n",
        "    relevant_test_indices = get_relevant_indices(testset, classes, target_classes)\n",
        "    test_sampler = SubsetRandomSampler(relevant_test_indices)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                             num_workers=1, sampler=test_sampler)\n",
        "    return train_loader, val_loader, test_loader, classes\n",
        "\n",
        "###############################################################################\n",
        "# Training\n",
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing the hyperparameters\n",
        "    Returns:\n",
        "        path: A string with the hyperparameter name and value concatenated\n",
        "    \"\"\"\n",
        "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
        "                                                   batch_size,\n",
        "                                                   learning_rate,\n",
        "                                                   epoch)\n",
        "    return path\n",
        "\n",
        "def normalize_label(labels):\n",
        "    \"\"\"\n",
        "    Given a tensor containing 2 possible values, normalize this to 0/1\n",
        "\n",
        "    Args:\n",
        "        labels: a 1D tensor containing two possible scalar values\n",
        "    Returns:\n",
        "        A tensor normalize to 0/1 value\n",
        "    \"\"\"\n",
        "    max_val = torch.max(labels)\n",
        "    min_val = torch.min(labels)\n",
        "    norm_labels = (labels - min_val)/(max_val - min_val)\n",
        "    return norm_labels\n",
        "\n",
        "def evaluate(net, loader, criterion):\n",
        "    \"\"\" Evaluate the network on the validation set.\n",
        "\n",
        "     Args:\n",
        "         net: PyTorch neural network object\n",
        "         loader: PyTorch data loader for the validation set\n",
        "         criterion: The loss function\n",
        "     Returns:\n",
        "         err: A scalar for the avg classification error over the validation set\n",
        "         loss: A scalar for the average loss function over the validation set\n",
        "     \"\"\"\n",
        "    total_loss = 0.0\n",
        "    total_err = 0.0\n",
        "    total_epoch = 0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        inputs, labels = data\n",
        "        labels = normalize_label(labels)  # Convert labels to 0/1\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        corr = (outputs > 0.0).squeeze().long() != labels\n",
        "        total_err += int(corr.sum())\n",
        "        total_loss += loss.item()\n",
        "        total_epoch += len(labels)\n",
        "    err = float(total_err) / total_epoch\n",
        "    loss = float(total_loss) / (i + 1)\n",
        "    return err, loss\n",
        "\n",
        "###############################################################################\n",
        "# Training Curve\n",
        "def plot_training_curve(path):\n",
        "    \"\"\" Plots the training curve for a model run, given the csv files\n",
        "    containing the train/validation error/loss.\n",
        "\n",
        "    Args:\n",
        "        path: The base path of the csv files produced during training\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
        "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
        "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
        "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
        "    plt.title(\"Train vs Validation Error\")\n",
        "    n = len(train_err) # number of epochs\n",
        "    plt.plot(range(1,n+1), train_err, label=\"Train\")\n",
        "    plt.plot(range(1,n+1), val_err, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Error\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    plt.title(\"Train vs Validation Loss\")\n",
        "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
        "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxMgWGNqID2"
      },
      "source": [
        "## Part 1. Visualizing the Data [7 pt]\n",
        "\n",
        "We will make use of some of the CIFAR-10 data set, which consists of\n",
        "colour images of size 32x32 pixels belonging to 10 categories. You can\n",
        "find out more about the dataset at https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "For this assignment, we will only be using the cat and dog categories.\n",
        "We have included code that automatically downloads the dataset the\n",
        "first time that the main script is run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp7LVcGfqID3",
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# This will download the CIFAR-10 dataset to a folder called \"data\"\n",
        "# the first time you run this code.\n",
        "train_loader, val_loader, test_loader, classes = get_data_loader(\n",
        "    target_classes=[\"cat\", \"dog\"],\n",
        "    batch_size=1) # One image per batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiYX_HM-qID6"
      },
      "source": [
        "### Part (a) -- 1 pt\n",
        "\n",
        "Visualize some of the data by running the code below.\n",
        "Include the visualization in your writeup.\n",
        "\n",
        "(You don't need to submit anything else.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ4pYFTQqID7",
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "k = 0\n",
        "for images, labels in train_loader:\n",
        "    # since batch_size = 1, there is only 1 image in `images`\n",
        "    image = images[0]\n",
        "    # place the colour channel at the end, instead of at the beginning\n",
        "    img = np.transpose(image, [1,2,0])\n",
        "    # normalize pixel intensity values to [0, 1]\n",
        "    img = img / 2 + 0.5\n",
        "    plt.subplot(3, 5, k+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "\n",
        "    k += 1\n",
        "    if k > 14:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmHiUOaDqID9"
      },
      "source": [
        "### Part (b) -- 3 pt\n",
        "\n",
        "How many training examples do we have for the combined `cat` and `dog` classes?\n",
        "What about validation examples?\n",
        "What about test examples?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p4eAz1IqID-",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "2e569c3c-2e3f-4716-cdaa-b881275073f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 8000 training examples for the combined at and dog classes.\n",
            "There are 2000 validation examples for the combined at and dog classes.\n",
            "There are 2000 test examples for the combined at and dog classes.\n"
          ]
        }
      ],
      "source": [
        "print(\"There are\",len(train_loader),\"training examples for the combined at and dog classes.\")\n",
        "print(\"There are\",len(val_loader),\"validation examples for the combined at and dog classes.\")\n",
        "print(\"There are\",len(test_loader),\"test examples for the combined at and dog classes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aY25I_oqIEB"
      },
      "source": [
        "### Part (c) -- 3pt\n",
        "\n",
        "Why do we need a validation set when training our model? What happens if we judge the\n",
        "performance of our models using the training set loss/error instead of the validation\n",
        "set loss/error?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> We need a validation set when training our model in order to avoid\n",
        "overfitting to the testing data and underperforming on unseen data. We need a way to verify the outputs of our model and adjust the configuration accordingly. After being trained, the validation set loss/error is used to determine which hyperparameters need to be tuned to best improve the model.\n",
        "Judging the performance of our model using the training set loss/error\n",
        "instead of the validation set loss/error can be problematic in the case where\n",
        "the training set loss/error is very low, even though the model is not ideal. It\n",
        "needs to be tested on data it hasn't seen before, or else overfitting will\n",
        "happen and the model will perform poorly on new data. Using the validation set\n",
        "however avoids these problems as the model's performance is reflected in the\n",
        "loss/error being low. Thus we can conclude the model is performing well."
      ],
      "metadata": {
        "id": "91K04MqUXkwN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iYZ4xXHqIEE"
      },
      "source": [
        "## Part 2. Training [15 pt]\n",
        "\n",
        "We define two neural networks, a `LargeNet` and `SmallNet`.\n",
        "We'll be training the networks in this section.\n",
        "\n",
        "You won't understand fully what these networks are doing until\n",
        "the next few classes, and that's okay. For this assignment, please\n",
        "focus on learning how to train networks, and how hyperparameters affect\n",
        "training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HniZoI3lqIEF",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "class LargeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LargeNet, self).__init__()\n",
        "        self.name = \"large\"\n",
        "        self.conv1 = nn.Conv2d(3, 5, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(5, 10, 5)\n",
        "        self.fc1 = nn.Linear(10 * 5 * 5, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 10 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = x.squeeze(1) # Flatten to [batch_size]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pJgOmJdvqIEH",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "class SmallNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SmallNet, self).__init__()\n",
        "        self.name = \"small\"\n",
        "        self.conv = nn.Conv2d(3, 5, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc = nn.Linear(5 * 7 * 7, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv(x)))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 5 * 7 * 7)\n",
        "        x = self.fc(x)\n",
        "        x = x.squeeze(1) # Flatten to [batch_size]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pMJZt_S2qIEM",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "small_net = SmallNet()\n",
        "large_net = LargeNet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ekO3pj9qIEQ"
      },
      "source": [
        "### Part (a) -- 2pt\n",
        "\n",
        "The methods `small_net.parameters()` and `large_net.parameters()`\n",
        "produces an iterator of all the trainable parameters of the network.\n",
        "These parameters are torch tensors containing many scalar values.\n",
        "\n",
        "We haven't learned how how the parameters in these high-dimensional\n",
        "tensors will be used, but we should be able to count the number\n",
        "of parameters. Measuring the number of parameters in a network is\n",
        "one way of measuring the \"size\" of a network.\n",
        "\n",
        "What is the total number of parameters in `small_net` and in\n",
        "`large_net`? (Hint: how many numbers are in each tensor?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc4keyOnqIER",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "bda30e8d-1374-4533-8941-2384eee08b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3, 3, 3])\n",
            "torch.Size([5])\n",
            "torch.Size([1, 245])\n",
            "torch.Size([1])\n",
            "\n",
            "The number of parameters in small_net is 5*3*3*3 + 5 + 1*245 + 1 = 386.\n",
            "\n",
            "torch.Size([5, 3, 5, 5])\n",
            "torch.Size([5])\n",
            "torch.Size([10, 5, 5, 5])\n",
            "torch.Size([10])\n",
            "torch.Size([32, 250])\n",
            "torch.Size([32])\n",
            "torch.Size([1, 32])\n",
            "torch.Size([1])\n",
            "\n",
            "The number of parameters in large_net is 5*3*3*3 + 5 + 10*5*5*5 + 10 + 32*250 + 32 + 1*32 + 1 = 9705.\n"
          ]
        }
      ],
      "source": [
        "for param in small_net.parameters():\n",
        "    print(param.shape)\n",
        "\n",
        "print(\"\\nThe number of parameters in small_net is 5*3*3*3 + 5 + 1*245 + 1 = 386.\\n\")\n",
        "\n",
        "for param in large_net.parameters():\n",
        "    print(param.shape)\n",
        "\n",
        "print(\"\\nThe number of parameters in large_net is 5*3*3*3 + 5 + 10*5*5*5 + 10 + 32*250 + 32 + 1*32 + 1 = 9705.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcZVo2DsqIET"
      },
      "source": [
        "### The function train_net\n",
        "\n",
        "The function `train_net` below takes an untrained neural network (like `small_net` and `large_net`) and\n",
        "several other parameters. You should be able to understand how this function works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jC90G_VGqIEU",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def train_net(net, batch_size=64, learning_rate=0.01, num_epochs=30):\n",
        "    ########################################################################\n",
        "    # Train a classifier on cats vs dogs\n",
        "    target_classes = [\"cat\", \"dog\"]\n",
        "    ########################################################################\n",
        "    # Fixed PyTorch random seed for reproducible result\n",
        "    torch.manual_seed(1000)\n",
        "    ########################################################################\n",
        "    # Obtain the PyTorch data loader objects to load batches of the datasets\n",
        "    train_loader, val_loader, test_loader, classes = get_data_loader(\n",
        "            target_classes, batch_size)\n",
        "    ########################################################################\n",
        "    # Define the Loss function and optimizer\n",
        "    # The loss function will be Binary Cross Entropy (BCE). In this case we\n",
        "    # will use the BCEWithLogitsLoss which takes unnormalized output from\n",
        "    # the neural network and scalar label.\n",
        "    # Optimizer will be SGD with Momentum.\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    ########################################################################\n",
        "    # Set up some numpy arrays to store the training/test loss/erruracy\n",
        "    train_err = np.zeros(num_epochs)\n",
        "    train_loss = np.zeros(num_epochs)\n",
        "    val_err = np.zeros(num_epochs)\n",
        "    val_loss = np.zeros(num_epochs)\n",
        "    ########################################################################\n",
        "    # Train the network\n",
        "    # Loop over the data iterator and sample a new batch of training data\n",
        "    # Get the output from the network, and optimize our loss function.\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "        total_train_loss = 0.0\n",
        "        total_train_err = 0.0\n",
        "        total_epoch = 0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # Get the inputs\n",
        "            inputs, labels = data\n",
        "            labels = normalize_label(labels) # Convert labels to 0/1\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass, backward pass, and optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Calculate the statistics\n",
        "            corr = (outputs > 0.0).squeeze().long() != labels\n",
        "            total_train_err += int(corr.sum())\n",
        "            total_train_loss += loss.item()\n",
        "            total_epoch += len(labels)\n",
        "        train_err[epoch] = float(total_train_err) / total_epoch\n",
        "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
        "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
        "        print((\"Epoch {}: Train err: {}, Train loss: {} |\"+\n",
        "               \"Validation err: {}, Validation loss: {}\").format(\n",
        "                   epoch + 1,\n",
        "                   train_err[epoch],\n",
        "                   train_loss[epoch],\n",
        "                   val_err[epoch],\n",
        "                   val_loss[epoch]))\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
        "        torch.save(net.state_dict(), model_path)\n",
        "    print('Finished Training')\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
        "    # Write the train/test loss/err into CSV file for plotting later\n",
        "    epochs = np.arange(1, num_epochs + 1)\n",
        "    np.savetxt(\"{}_train_err.csv\".format(model_path), train_err)\n",
        "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
        "    np.savetxt(\"{}_val_err.csv\".format(model_path), val_err)\n",
        "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEhligNaqIEW"
      },
      "source": [
        "### Part (b) -- 1pt\n",
        "\n",
        "The parameters to the function `train_net` are hyperparameters of our neural network.\n",
        "We made these hyperparameters easy to modify so that we can tune them later on.\n",
        "\n",
        "What are the default values of the parameters `batch_size`, `learning_rate`,\n",
        "and `num_epochs`?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The default values are batch_Size = 64, learning_Rate = 0.01, and num_epochs = 30.\n",
        "\n"
      ],
      "metadata": {
        "id": "ouHITzPGXq8Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy_yv9NMqIEa"
      },
      "source": [
        "### Part (c) -- 3 pt\n",
        "\n",
        "What files are written to disk when we call `train_net` with `small_net`, and train for 5 epochs? Provide a list\n",
        "of all the files written to disk, and what information the files contain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNwhtyP1qIEb",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "train_net(small_net, 64, 0.01, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The files written to disk when we call train_net with small_net, and train for 5 epochs are:\n",
        "*   model_small_bs64_lr0.01_epoch0 contains model checkpoint for epoch 1\n",
        "*   model_small_bs64_lr0.01_epoch1 contains model checkpoint for epoch 2\n",
        "*   model_small_bs64_lr0.01_epoch2 contains model checkpoint for epoch 3\n",
        "*   model_small_bs64_lr0.01_epoch3 contains model checkpoint for epoch 4\n",
        "*   model_small_bs64_lr0.01_epoch4 contains model checkpoint for epoch 5\n",
        "*   model_small_bs64_lr0.01_epoch4_train_err.csv contains training error per epoch\n",
        "*   model_small_bs64_lr0.01_epoch4_train_loss.csv contains training loss per epoch\n",
        "*   model_small_bs64_lr0.01_epoch4_val_err.csv contains validation error per epoch\n",
        "*   model_small_bs64_lr0.01_epoch4_val_loss.csv contains validation loss per epoch"
      ],
      "metadata": {
        "id": "dKHFy4na6Mk4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6shtCFDqIEd"
      },
      "source": [
        "### Part (d) -- 2pt\n",
        "\n",
        "Train both `small_net` and `large_net` using the function `train_net` and its default parameters.\n",
        "The function will write many files to disk, including a model checkpoint (saved values of model weights)\n",
        "at the end of each epoch.\n",
        "\n",
        "If you are using Google Colab, you will need to mount Google Drive\n",
        "so that the files generated by `train_net` gets saved. We will be using\n",
        "these files in part (d).\n",
        "(See the Google Colab tutorial for more information about this.)\n",
        "\n",
        "Report the total time elapsed when training each network. Which network took longer to train?\n",
        "Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tawIVwTfqIEe",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Since the function writes files to disk, you will need to mount\n",
        "# your Google Drive. If you are working on the lab locally, you\n",
        "# can comment out this code.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWSZm9SvqIEh",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "train_net(small_net)\n",
        "train_net(large_net)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> The total time elapsed when training small_net was 176.97 seconds.\n",
        "\n",
        "> The total time elapsed when training large_net was 183.91 seconds.\n",
        "\n",
        "> large_net took longer to traing because it has more parameters which need to be updated and a higher model complexity compared to small_net."
      ],
      "metadata": {
        "id": "5bVMhGQP-LjE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk4GIF3QqIEj"
      },
      "source": [
        "### Part (e) - 2pt\n",
        "\n",
        "Use the function `plot_training_curve` to display the trajectory of the\n",
        "training/validation error and the training/validation loss.\n",
        "You will need to use the function `get_model_name` to generate the\n",
        "argument to the `plot_training_curve` function.\n",
        "\n",
        "Do this for both the small network and the large network. Include both plots\n",
        "in your writeup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXrEgOtuqIEk",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "model_path = get_model_name(\"small\", batch_size=64, learning_rate=0.01, epoch=29)\n",
        "plot_training_curve(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = get_model_name(\"large\", batch_size=64, learning_rate=0.01, epoch=29)\n",
        "plot_training_curve(model_path)"
      ],
      "metadata": {
        "id": "OdT3Efmp_K__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUzWo8HRqIEu"
      },
      "source": [
        "### Part (f) - 5pt\n",
        "\n",
        "Describe what you notice about the training curve.\n",
        "How do the curves differ for `small_net` and `large_net`?\n",
        "Identify any occurences of underfitting and overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">  In the training curve for small_net, underfitting can be seen within the first few epochs. The small_net training error decreases rapidly at lower epochs compared to large_net. Both training error/loss and validation error/loss get smaller as a higher number of epochs is approached. There is a lot more fluctuation in the validation error/loss curve than the training error/loss curve.\n",
        "\n",
        "\n",
        "\n",
        "> The training curve for large_net shows that training error/loss curve decreases at a steadier rate as the number of epochs increases. The validation error/loss curve flattens out around half way through. This is a sign of overfitting as after around 15 epochs, the model is overfitted to the training data. The curves overall have a lot less noise/fluctuations compared to small_net."
      ],
      "metadata": {
        "id": "xqU97JSQ_SYB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYP_y58SqIEz"
      },
      "source": [
        "## Part 3. Optimization Parameters [12 pt]\n",
        "\n",
        "For this section, we will work with `large_net` only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1uGdKUIqIE1"
      },
      "source": [
        "### Part (a) - 3pt\n",
        "\n",
        "Train `large_net` with all default parameters, except set `learning_rate=0.001`.\n",
        "Does the model take longer/shorter to train?\n",
        "Plot the training curve. Describe the effect of *lowering* the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9-toERSqIE2",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Note: When we re-construct the model, we start the training\n",
        "# with *random weights*. If we omit this code, the values of\n",
        "# the weights will still be the previously trained values.\n",
        "large_net = LargeNet()\n",
        "train_net(large_net, learning_rate=0.001)\n",
        "model_path = get_model_name(\"large\", batch_size=64, learning_rate=0.001, epoch=29)\n",
        "plot_training_curve(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The model took approximately the same amount of time to train, about 3 seconds shorter than the original. Lowering the learning rate to 0.001 means that the size of each step was smaller. This results in a slower decrease in error/loss which makes it so that the model does not overfit at larger numbers of epochs. Small fluctuations in error/loss curves.\n"
      ],
      "metadata": {
        "id": "eRE5wEmvCpzq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5APYluUoqIE4"
      },
      "source": [
        "### Part (b) - 3pt\n",
        "\n",
        "Train `large_net` with all default parameters, except set `learning_rate=0.1`.\n",
        "Does the model take longer/shorter to train?\n",
        "Plot the training curve. Describe the effect of *increasing* the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY7GrjFxqIE5",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "large_net = LargeNet()\n",
        "train_net(large_net, learning_rate=0.1)\n",
        "model_path = get_model_name(\"large\", batch_size=64, learning_rate=0.1, epoch=29)\n",
        "plot_training_curve(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The model took more time to train, about 9 seconds more than the original and 12 seconds more than the previous with a learning rate of 0.001. Increasing the learning rate to 0.1 means the size of each step was larger. This results in a faster decrease in the error/loss and the model begins to overfit at a lower number of epochs. Large fluctuations in both error/loss curves."
      ],
      "metadata": {
        "id": "g00qrtN9Dc6K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMb-NLitqIE7"
      },
      "source": [
        "### Part (c) - 3pt\n",
        "\n",
        "Train `large_net` with all default parameters, including with `learning_rate=0.01`.\n",
        "Now, set `batch_size=512`. Does the model take longer/shorter to train?\n",
        "Plot the training curve. Describe the effect of *increasing* the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHRzqMAcqIE8",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "large_net = LargeNet()\n",
        "train_net(large_net, batch_size=512)\n",
        "model_path = get_model_name(\"large\", batch_size=512, learning_rate=0.01, epoch=29)\n",
        "plot_training_curve(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The model took less time to train, about 26 seconds less than the original, 23 seconds less than the previous with a learning rate of 0.001, and 35 seconds less than the previous with a learning rate of 0.1. Increasing the batch size means that less iterations occur per epoch. This resulted in a steady decrease of training and validation error/loss as the number of epochs increased. The model will be able to learn patterns over generalized data better since it has more access to data."
      ],
      "metadata": {
        "id": "GjIef5EUEsr8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwvN15IzqIE-"
      },
      "source": [
        "### Part (d) - 3pt\n",
        "\n",
        "Train `large_net` with all default parameters, including with `learning_rate=0.01`.\n",
        "Now, set `batch_size=16`. Does the model take longer/shorter to train?\n",
        "Plot the training curve. Describe the effect of *decreasing* the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0SKvrNGqIE-",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "large_net = LargeNet()\n",
        "train_net(large_net, batch_size=16)\n",
        "model_path = get_model_name(\"large\", batch_size=16, learning_rate=0.01, epoch=29)\n",
        "plot_training_curve(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The model took more time to train, about 87 seconds more than the original, 90 seconds more than the previous with a learning rate of 0.001, 78 seconds less than the previous with a learning rate of 0.1, and 113 less than the previous with a batch size of 512. Decreasing the batch size means that more iterations occur per epoch. This resulted in a very low training error/loss and resulted in overfitting as validatiion error plateaus as the number of epochs increases and validation loss increases as the number of epochs increase. This means the model will perform well on training data but not well on unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "nhtEM6cbGiEP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MkpZLc4qIFA"
      },
      "source": [
        "## Part 4. Hyperparameter Search [6 pt]\n",
        "\n",
        "### Part (a) - 2pt\n",
        "\n",
        "Based on the plots from above, choose another set of values for the hyperparameters (network, batch_size, learning_rate)\n",
        "that you think would help you improve the validation accuracy. Justify your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Based on the plots from above I think the following values for the hyperparameters would help improve validation accuracy:\n",
        "\n",
        "\n",
        "> Netwok: large_net since it performed well in part 2e with few fluctuations/noise, one does have to be cautious of overfitting at a large number of epochs.\n",
        "\n",
        "\n",
        "> Batch size: 256 since results from 3a and 3c showed that increasing the batch size helped to reduce overfitting at an increased number of epochs. Steady decrease in validation/training error/loss as number of epochs increase. However I will am reducing from a batch size of 512 to 256 to improve runtime.\n",
        "\n",
        "\n",
        "> Learning rate: 0.001 since results from 3a and 3c showed that decreasing the learning rate helped to reduce overfitting at an increased number of epochs. Steady decrease in validation/training error/loss as number of epochs increase.\n",
        "\n",
        "\n",
        "> epochs: 30 since overall the error was small for a larger number of epochs. Could be icnreased a little bit more, maybe by 10 but need to be careful of overfitting."
      ],
      "metadata": {
        "id": "OEtEP9cWIe2_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTLRs2f7qIFD"
      },
      "source": [
        "### Part (b) - 1pt\n",
        "\n",
        "Train the model with the hyperparameters you chose in part(a), and include the training curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCFV10I9qIFD",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "large_net = LargeNet()\n",
        "train_net(large_net, batch_size=256, learning_rate=0.001)\n",
        "model_path_large = get_model_name(\"large\", batch_size=256, learning_rate=0.001, epoch=29)\n",
        "plot_training_curve(model_path_large)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsO4z7JJqIFF"
      },
      "source": [
        "### Part (c) - 2pt\n",
        "\n",
        "Based on your result from Part(a), suggest another set of hyperparameter values to try.\n",
        "Justify your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Based on the results from Part (a), I will keep the network the same as large_net and keep the batch size as 256, as increasing the batch size helped with overfitting, however it also reduced the rate at which the error decreased. To compensate for this, I will increase the learning rate to 0.05. This will then cause some overfitting at a larger number of epochs, however reducing the number of epochs to 25 will reduce this.\n"
      ],
      "metadata": {
        "id": "j4l7tqBWYkd6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrMyc5QnqIFI"
      },
      "source": [
        "### Part (d) - 1pt\n",
        "\n",
        "Train the model with the hyperparameters you chose in part(c), and include the training curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU0a0PHFqIFJ",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "large_net = LargeNet()\n",
        "train_net(large_net, batch_size=256, learning_rate=0.005, num_epochs=25)\n",
        "model_path_large = get_model_name(\"large\", batch_size=256, learning_rate=0.005, epoch=24)\n",
        "plot_training_curve(model_path_large)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kQB1IFuqIFN"
      },
      "source": [
        "## Part 5. Evaluating the Best Model [15 pt]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxje12xKqIFN"
      },
      "source": [
        "### Part (a) - 1pt\n",
        "\n",
        "Choose the **best** model that you have so far. This means choosing the best model checkpoint,\n",
        "including the choice of `small_net` vs `large_net`, the `batch_size`, `learning_rate`,\n",
        "**and the epoch number**.\n",
        "\n",
        "Modify the code below to load your chosen set of weights to the model object `net`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AanLwIAEqIFO",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "net = LargeNet()\n",
        "train_net(net, batch_size=256, learning_rate=0.005, num_epochs=25)\n",
        "model_path = get_model_name(net.name, batch_size=256, learning_rate=0.005, epoch=24)\n",
        "state = torch.load(model_path)\n",
        "net.load_state_dict(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXXwLMIXqIFS"
      },
      "source": [
        "### Part (b) - 2pt\n",
        "\n",
        "Justify your choice of model from part (a)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Network: large_net based on results from 2e and 2f\n",
        "\n",
        "> Batch size: 25 from results in 3a and 4c/4d because a large batch size reduces overfitting at a larger number of epochs and improves training time\n",
        "\n",
        "> Learning rate: 0.005 so the error/loss steadily decreases and avoids overfitting\n",
        "\n",
        "> Epochs: 25 since the model starts overfitting at a large number of epochs.\n"
      ],
      "metadata": {
        "id": "etcufCr1f4cX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0GE8Z5kqIFU"
      },
      "source": [
        "### Part (c) - 2pt\n",
        "\n",
        "Using the code in Part 0, any code from lecture notes, or any code that you write,\n",
        "compute and report the **test classification error** for your chosen model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPUe9iomqIFV",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27029afe-1a25-401a-8943-5d4081b2a298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "The test classification error is 0.356 and the test classification loss is 0.6410455703735352\n"
          ]
        }
      ],
      "source": [
        "# If you use the `evaluate` function provided in part 0, you will need to\n",
        "# set batch_size > 1\n",
        "train_loader, val_loader, test_loader, classes = get_data_loader(\n",
        "    target_classes=[\"cat\", \"dog\"],\n",
        "    batch_size=256)\n",
        "test_error, test_loss = evaluate(net, test_loader, nn.BCEWithLogitsLoss())\n",
        "print(\"The test classification error is\", test_error, \"and the test classification loss is\", test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXK0XaayqIFY"
      },
      "source": [
        "### Part (d) - 3pt\n",
        "\n",
        "How does the test classification error compare with the **validation error**?\n",
        "Explain why you would expect the test error to be *higher* than the validation error."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader, classes = get_data_loader(\n",
        "    target_classes=[\"cat\", \"dog\"],\n",
        "    batch_size=256)\n",
        "val_error, val_loss = evaluate(net, val_loader, nn.BCEWithLogitsLoss())\n",
        "print(\"The validation error is\", val_error, \"and the validation loss is\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvdoye-ZiIWm",
        "outputId": "676d780f-7d47-4481-835a-1c8193edb68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "The validation error is 0.372 and the validation loss is 0.638019360601902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The test classification error is slightly lower than the validation error. This suggests that the model performed a little bit better on the test data set than the classification dataset. This is unexpected since the model's parameters do not get updated after training on the test data, I would expect the test error to be higher than the validation error. However, since I was adjusting the hyperparameters with the itnention of maximizing validation accuracy, this does not lead to the best testing accuracy. Thus explaining why the validation error is a bit higher than the classification error.\n",
        "\n"
      ],
      "metadata": {
        "id": "rM7DXTanhRxc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMA9b8vXqIFd"
      },
      "source": [
        "### Part (e) - 2pt\n",
        "\n",
        "Why did we only use the test data set at the very end?\n",
        "Why is it important that we use the test data as little as possible?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> It is important that we put test data away and don’t look at it so we, as a the designer, don’t have cognitive bias about the test data. We save the test data till the very end because it is used to give as an accurate assessment of our model's performance. Building the model and adjusting the parameters based on the testing data increases the bias towards the test data and causes the model to overfit and start memorizing the answers.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ru9eTZ0mjqym"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxG0sYDZM2KR"
      },
      "source": [
        "### Part (f) - 5pt\n",
        "\n",
        "How does the your best CNN model compare with an 2-layer ANN model (no convolutional layers) on classifying cat and dog images. You can use a 2-layer ANN architecture similar to what you used in Lab 1. You should explore different hyperparameter settings to determine how well you can do on the validation dataset. Once satisified with the performance, you may test it out on the test data.\n",
        "\n",
        "Hint: The ANN in lab 1 was applied on greyscale images. The cat and dog images are colour (RGB) and so you will need to flatted and concatinate all three colour layers before feeding them into an ANN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pjI2Zw9jqIFg",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "class Pigeon(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Pigeon, self).__init__()\n",
        "    self.name = \"pigeon\"\n",
        "    self.layer1 = nn.Linear(32 * 32 * 3, 30)\n",
        "    self.layer2 = nn.Linear(30, 1)\n",
        "\n",
        "  def forward(self, img):\n",
        "    flattened = img.view(-1, 32 * 32 * 3)\n",
        "    activation1 = self.layer1(flattened)\n",
        "    activation1 = F.relu(activation1)\n",
        "    activation2 = self.layer2(activation1)\n",
        "    activation2 = activation2.squeeze(1)\n",
        "    return activation2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing:"
      ],
      "metadata": {
        "id": "K_auREmXlKch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pigeon = Pigeon()\n",
        "train_net(pigeon, batch_size=512, learning_rate=0.001)\n",
        "model_path = get_model_name(\"pigeon\", batch_size=512, learning_rate=0.001, epoch=29)\n",
        "plot_training_curve(model_path)"
      ],
      "metadata": {
        "id": "Jt0jrgc2lISi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training:"
      ],
      "metadata": {
        "id": "UtF0oNuklLu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader, classes = get_data_loader(\n",
        "  target_classes=[\"cat\", \"dog\"],\n",
        "  batch_size=512)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "err, loss = evaluate(pigeon, test_loader, criterion)\n",
        "print(\"The test classification error is\", err, \"and the test loss is\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLoJXJa8k0xg",
        "outputId": "a5eda392-1608-40f2-e990-5fde3a4217fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "The test classification error is 0.3765 and the test loss is 0.6487362831830978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> The 2-layer ANN model has a much higher test classification error/loss compared to that of CNN model. Thus the CNN model is much better suited for classifying cats versus dogs since it fits better with the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "BwLzqvPEllTy"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}